\chapter{Analysis}\label{ch.an}

Recalling from [somewhere] that we may express the number of predicted events in a given bin in the $M_{\gamma\gamma}$ as a second order polynomial in $\Lambda^4$, our aim here will be, first, to determine, for each bin, the coefficients of this polynomial. Second, using a maximum likelihood fit, to determine a most likely value of $\Lambda$, and a confidence interval on that value.

\section{Corrections to Monte Carlo samples}

In chapter~\ref{ch.mc}, we produced, among others, a sample of Monte Carlo events that gives the SM prediction for the distribution of events. Since we discovered above, that the \atlas{} $\gamma\gamma$ Monte Carlo sample gives a distribution of events that matches data well, we will compare these two samples to asses how well our SM prediction matches data. In doing so, we encounter a few problems.

First, it appears that the procedure which should correct for pileup in the detector simulation procedure has not functioned as intended. During detector simulation, the events produced for this thesis are assigned only a very limited range of values for the number of interactions per bunch crossing. Pileup reweighting produces a weight for each event so that its distribution in a Monte Carlo dataset mathches the distribution found in data.

Since this appears to be a technical glitch, we will substitute a reweighting in the number of reconstructed primary vertices---vertices considered to originate directly from interactions between protons---in each event. The distribution of numbers of primary vertices in the present MC data set is compared to the one found in the \atlas{} MC set in figure~\ref{pvnnone}.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\input{figures/pvnnone}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{The distribution of the number of reconstructed primary vertices in the \atlas{} $\gamma\gamma$ MC set and in the CalcHEP MC set produced for this thesis, normalised to the same number of events.}\label{pvnnone}
\end{minipage}
\end{figure}


The second issue we encounter is in the distribution of $E_T^\text{iso}$, which is much broader in the CalcHEP MC set than in the \atlas{} one, as illustrated in figure~\ref{etpv}. Assuming that the distribution in the CalcHEP sample is just the one in the \atlas{} sample, but broadened and shifted slightly, we develop a mapping function to reverse that effect [\~$E_T^\text{iso}/4+0.2$ GeV]. The result of applying this function is shown in fig.~\ref{etmap}.

\begin{figure}[htp]
\begin{minipage}[b]{.49\textwidth}
\begin{infilsf} \tiny 
\input{figures/etpv}
\end{infilsf}
\end{minipage}
\hfill
\begin{minipage}[b]{.49\textwidth}
\begin{infilsf} \tiny 
\input{figures/etmap}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.49\textwidth}
\subcaption{Before application of mapping function.\label{etpv}}
\end{minipage}\hfill
\begin{minipage}[b]{.49\textwidth}
\subcaption{After application of mapping function.\label{etmap}}
\end{minipage}
\caption{The distribution of $E_T^\text{iso}$ in the \atlas{} MC set compared with the distribution in the CalcHEP MC set. In \subcaptionref{etmap}, a mapping function which applies a scale and offset to the values for the CalcHEP MC has been applied.}
\end{figure}

These distributions are now very close to being identical. We correct the remaining discrepancy by reweighting the CalcHEP sample.

The weights found above are assigned to the Monte Carlo samples on an event--by--event basis. These weights will of course carry an uncertainty, which can be derived by simple error propagation from the uncertainties of the histograms used to define them. As these weights are assigned event--by--event, we can track the magnitude of uncertainty assigned to events in each bin of $M_{\gamma\gamma}$, which must then be included in the systematic uncertainties on the analysis.

Finally, the CalcHEP sample only included events produced by the tree level process, whereas the \atlas{} sample also includes the contribution from the box diagram shown in fig.~\ref{hiorder}. So, to meaningfully compare the two, we must know the contribution from the box diagram. This, we glean from another \atlas{} MC sample\footnote{See appendix \ref{ax.ggb}}, which provides a $M_{\gamma\gamma}$ distribution illustrated in fig.~\ref{boxmgg}. As with the estimated background, this distribution has insufficient statistics to accurately represent the shape of the distribution in the interesting region above 1\,000 GeV, forcing us to extrapolate the shape of this distribution as well.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\input{figures/boxmgg}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{The distribution of invariant masses in the \atlas{} box diagram data set.}\label{boxmgg}
\end{minipage}
\end{figure}

Adding this to the CalcHEP sample, we get the distribution in fig.~\ref{ggcomp}. As should be evident, there is still a deficit in the CalcHEP sample. The box diagram contribution added to the CalcHEP sample was produced by a different generator than was used to generate the \atlas{} $\gamma\gamma$ sample, which was not necessarily configured with the same parameters as was used for the present sample, or the \altas{} sample could contain effects from interference between the diagrams which simply adding together samples does not capture. In any case, to be comparable to data, both MC sets must be combined with the $\gamma$jet sample. The two combined samples will be consistent within their errors.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\input{figures/ggcomp}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{Comparing the \atlas{} distribution with the one produced by CalcHEP, combined with a distribution for the box diagram contribution.}\label{ggcomp}
\end{minipage}
\end{figure}

As the data background and box diagram distributions have the same sort of shape, it is natural to combine them before attempting to extrapolate a shape. The function we fit to this distribution has the expression
\(f(x)=\vet{7.9\,10^{-5}}\left(\frac{1-x}{8000}\right)^{\vet{22.5506}}\left(\frac{x}{8000}\right)^{-\left[\vet{7.203553}\;\vet{-0.631809}\ln\frac{x}{8000}\right]},\)
where the four underscored numbers are the fitted parameters. $x$ has been scaled by $8\,000^{-1}$ in an attempt to achieve parameter values close to 1. The function is plotted along with the histogram it was fitted to in figure~\ref{bckfit}. This fit, and all further fits, were carried out using \textsc{root}s fitting procedures \cite{root}.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\input{figures/bckfit}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{Extrapolating the truncated backgrounds by fitting a function.}\label{bckfit}
\end{minipage}
\end{figure}

\section{The polynomial coefficients}
Now satisfied with our Monte Carlo samples, we turn toward extracting the coefficients to the polynomials that will allow us to vary $\Lambda$. As is evident from the plots of the distributions in figure~\ref{simfit}, the number of expected events in each bin is subject to relatively large statistical fluctuations. Since the coefficients for each polynomial is determined solely from these three points, the resulting quadratic function may look drastically different from bin to bin. One approach to reducing the statistical fluctuations is to simply combine sets of adjacent bins, rebinning, or to apply some form of smoothing procedure. Here however, we will attempt to fit a function to each distribution. In addition to smoothing out fluctuations between bins, using functions allows us to incorporate some additional knowledge about the relationship between these distributions into our extrapolation of their shape.

Since we have assumed constructive interference between the new interaction and the Standard Model, we know that a distribution which has a contribution from the new interaction, at any strength, must have a number of expected events at least equal to the number of events expected in the Standard Model case at any point. Also, the Standard Model contributes equally in each distribution, thus the shape of each distribution must be the shape of the Standard Model contribution plus an additional contribution. We select for the shape of the Standard Model distribution the function
\(f_\textit{SM}(x)=p_1\left(\frac{1-x}{8000}\right)^{p_2}\left(\frac{x}{8000}\right)^{-\left[p_3+p_4\ln\frac{x}{8000}}\right],\)
the parameters of which must be shared among all three functions. This is the same type of expression as was used to fit the shape of the background distribution in the previous section.

We describe the shape of the contribution from the new interaction with functions of the form
\(f_{\Lambda=1.00/0.75}(x)=f_\textit{SM}(x)+p_{5/10}\exp\left[-\half\left(\frac{x-p_{6/11}}{p_{7/12}}\right)+p_{8/13}\frac{x}{8000}-p_{9/14}\right],\)
where we have attempted to indicate the parameter numbers belonging to the function describing the contribution at $\Lambda=1.00$ TeV resp. 0.75 TeV with two indexes separated by a slash. This is effectively a gaussian plus an exponential. We fit the distributions in the range above 150 GeV, which clears the effects of the minimum $p_T$ cuts, and below 3\,000 GeV, where the SM sample runs out of statistics.

The results of fitting these functions simultaneously is shown in figure~\ref{simfit}.


\begin{figure}[hbt]
\begin{infilsf}\tiny
\input{figures/simfit}
\end{infilsf}
\caption{The functions fitted to each of the CalcHEP distributions with different $\Lambda$ values of 0.75 TeV (yellow), 1.00 TeV (red) and $\infty$ (the SM, green). The fit is carried out in the range $150 \le M_{\gamma\gamma} \le 3\,000$ TeV.}\label{simfit}
\end{figure}

The fit estimates the parameters to be:\allowdisplaybreaks
\begin{align*}
p_1                      & \quad = &  7.80556\,10^{-6}   \quad & \pm    3.13184\,10^{-7} \\
p_2                      & \quad = &      6.96084   \quad & \pm    0.135114    \\
p_3                      & \quad = &      6.89429   \quad & \pm    0.018134    \\
p_4                      & \quad = &    -0.553642   \quad & \pm    0.00340326  \\
p_5                      & \quad = &     0.013996   \quad & \pm    0.00216626  \\
p_6                      & \quad = &      2540.87   \quad & \pm    61.1552     \\
p_7                      & \quad = &      476.819   \quad & \pm    96.2365     \\
p_8                      & \quad = &     -11.6121   \quad & \pm    0.567273    \\
p_9                      & \quad = &     0.125789   \quad & \pm    0.0905248   \\
p_{10}                     & \quad = &     0.193059   \quad & \pm    0.0284315   \\
p_{11}                     & \quad = &      2199.72   \quad & \pm    60.9718     \\
p_{12}                     & \quad = &      740.393   \quad & \pm    86.1043     \\
p_{13}                     & \quad = &     -7.58818   \quad & \pm    0.749441    \\
p_{14}                     & \quad = &    -0.389471   \quad & \pm    0.0800421   
\end{align*}

The error bands in fig.~\ref{simfit} represent the 95\% confidence intervals on the functions, as calculated from the covariance matrix produced by the fitting procedure. This confidence interval represents the systematic uncertainty that arises from the uncertainty on the fit parameters.

It is now a simple matter to determine the polynomial coefficients.

\section{Maximum likelihood fit}
To determine the most likely value of $\Lambda$, we will use a maximum likelihood fit to attempt to find the value of $\Lambda$ that best fits the data.

A full description of the maximum likelihood method is given in \cite{barlow} or \cite{pdg}. In brief, the probability of observing a number $n$ events, given an expected number $N_\text{exp}$, is given by Poisson statistics:
\(p(n|N_\text{exp}=\frac{N_\text{exp}^n}{n!}e^{-N_\text{exp}}\label{pois}\)
In our case, where the experiment has been carried out, $n$ will be fixed to the value in each bin. Meanwhile, we worked out a method for varying the number of expected events above. Thus, the most likely value of $\Lambda$ is the one that maximises equation \eqref{pois}. Taking the logarithm of eq.~\eqref{pois} yelds
\(\ln[p(n|N_\text{exp})]=-\ln n!+n\ln N_\text{exp}-N\text{exp},\)
where obviously the logarithm of $n!$ is a constant for any given bin. Taking the logarithm also allows us to write the likelihood of an ensemble of measured and expected event counts, such as we find in our histograms, as a sum. Finding the value of $\Lambda$ with the highest likelihood associated is now a simple matter of varying our prediction until we discover a maximum.

As a practical matter, since numerical algorithms for extremum finding are actually minimisers, we will write the likelihood as 
\[-\ln p,\]
and attempt to find a value of $\Lambda$ that minimises this number. 

We can find a confidence interval around the most likely value by determining what the ratio of likelihoods, or difference of log--likelihoods, between the most likely value and the value that corresponds to the desired confidence level. The log ratio of the likelihood of a $\chi^2$ distribution at its mean to the likelihood of a $\chi^2$ distribution at the 95th percentile is 1.92 \cite{pdg}. We adopt this as the best estimate available for the 95\% confidence level for our distributions as well.

Additional uncertainties can be incorporated into the determination of the log--likelihood ratio by introducing nuisance parameters. Say that, in addition to the likelihood associated with the Poisson distribution of observed events, the likelihood of a given expected event count also depends on a second distribution $\mu$. Then, to find the likelihood at a step in $N_\text{exp}$, we must find the value of $\mu$ that minimises the total likelihood.

\subsection{Systematic uncertainties}
In the course of developing the MC sets used, we identified a number of systematic uncertainties that should be included at this point.

To summarise, they were:
\begin{itemize}
\item Confidence interval on $M_{\gamma\gamma}$ shape functions: Potentially, these could be evaluated separately for each value of $\Lambda$. This is too cumbersome to implement, so we take as a conservative estimate the confidence interval associated with a function constructed for $\Lambda=0.76$, which is the lowest value searched by the minimiser.
\item Choice of event generator: We have attempted to minimise the effect of generator choice by actively tuning CalcHEP to produce distributions of events similar to pythia. A systematic error of 9.18\% must be included to account for the deviation of the MadGraph sample.
\item Choice of PDF: Table \ref{mrsttab} summarises the uncertainties determined due to our choice of PDF. Since we have identified separate errors for each $\Lambda$ sample, we will apply this effect to the appropriate $M_{\gamma\gamma}$ function before determining the coefficients.
\item Reweighting: The reweighting procedure allowed us to determine individual uncertainties for each bin in $M_{\gamma\gamma}$.
\end{itemize}
In addition, we include systematic uncertanties due to the following detector effects:
\begin{itemize}
\item Photon identification: 1.5\%
\item Uncertainty on luminosity: 2.8\%
\end{itemize}

We will, in the absence of better information, assume that all systematic uncertainties are uncorrelated, and, unless otherwise noted, that individual uncertainties are completely correlated across all bins. These uncertainties will then be included in the log--likelihood fit by constructing a simple Gaussian distribution of appropriate width for each, and then combining them. In practice, this means adding together their standard deviations in quadrature. For the confidence interval on the fitted functions and the PDF uncertainty, the errors are arrived at by propagating the error associated with the initial functions through the process that determines the coefficients, and then the process that constructs the distribution for $\Lambda=0.76$ from those coefficients.

[Depending on computer time]
For the background sample, by far the dominant source of uncertainty is that arising from limited statistics in the $\gamma$jet sample. As such

\section{Setting a limit}
[This section will contain at leat one plot of the ll ratio, a plot of the reconstructed mgg distribution overlaid on data, and the all important FINAL NUMBER!

When they're ready]

