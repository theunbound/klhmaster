% !TEX program = xelatex
\input{prekux.tex}

\begin{document}
\begin{english}
\begin{titlingpage}
{
%\definecolor{kugray}{RGB}{102,102,102}
%\definecolor{natgreen}{RGB}{50,93,61}
\thispagestyle{empty}
\newlength{\topma}\setlength{\topma}{-1in}\addtolength{\topma}{-\headsep}\addtolength{\topma}{-\voffset}\addtolength{\topma}{11mm}
\newlength{\sidema}\setlength{\sidema}{-1in}\addtolength{\sidema}{-\hoffset}\addtolength{\sidema}{-\oddsidemargin}\addtolength{\sidema}{-\marginparsep}\addtolength{\sidema}{15mm}
\newlength{\textwa}\setlength{\textwa}{\paperwidth}\addtolength{\textwa}{-35mm}\addtolength{\textwa}{-\textwidth}
\newlength{\textha}\setlength{\textha}{-\textheight}\addtolength{\textha}{\paperheight}\addtolength{\textha}{-11mm}\addtolength{\textha}{-.1\paperheight}
\changepage{\textha}{\textwa}{}{\sidema}{}{-\topmargin}{-\headheight}{\topma}{}
%\setlength{\parindent}{0pt}
\noindent\begin{minipage}[t]{.8\textwidth}
\noindent\raggedright \textcolor{kugray}{\fontspec[Path=fonts/garamond/]{GaramondPremrPro.otf}\addfontfeature{LetterSpace=13.0}\fontsize{18}{17}\selectfont\textsc{niels bohr institute}}

\vspace{.2em}\textcolor{kugray}{\fontspec[Path=fonts/garamond/]{GaramondPremrPro.otf}\addfontfeature{LetterSpace=13.0}\fontsize{15}{17}\selectfont\textsc{university of copenhagen}}
\end{minipage}\hfill\begin{minipage}[t]{32mm}\raggedleft \vspace{16mm} \includegraphics[height=44mm]{figures/bionat.pdf}
\end{minipage}


\newlength{\markbump}\setlength{\markbump}{.3\paperwidth}\addtolength{\markbump}{-6mm}
\newlength{\markdown}\setlength{\markdown}{-75mm}\addtolength{\markdown}{0em}\addtolength{\markdown}{\paperheight}\addtolength{\markdown}{.15\paperwidth}
\noindent\makebox[0pt][l]{\hspace{\markbump}\raisebox{-\markdown}[0pt][0pt]{\includegraphics[height=.85\paperwidth]{figures/atlaskugrid2.pdf}}}\makebox[0pt][r]{\raisebox{3.2mm}[0pt][0pt]{\textcolor{natgreen}{\rule{.06\paperwidth}{.7pt}}}}\makebox[0pt][l]{\raisebox{3.2mm}[0pt][0pt]{\textcolor{natgreen}{\rule{.95\paperwidth}{.7pt}}}}





\bigskip

{\sffamily
{\textbf{ }

\vspace{2em}}

{\huge \textbf{Master's thesis}

\vspace{.3em}}

{\Large Kristoffer Levin Hansen}

\vspace{3em}

{\Huge Searching for {\fontspec[Path=fonts/dejavu/,Scale=MatchLowercase,BoldItalicFont={DejaVuSansCondensed-BoldOblique.ttf},BoldFont=DejaVuSansCondensed-Bold.ttf,ItalicFont=DejaVuSans-Oblique.ttf]{DejaVuSansCondensed.ttf}\textit{γγ}} contact interactions with 

the ATLAS detector at the LHC}
\vspace{7em}

{\Large Academic advisor: Jørgen Beck Hansen}
\vfill

\today}
\clearpage}
\thispagestyle{empty}
  \phantom{p}
\vspace{1.16\textwidth}

\begin{center}
\includegraphics[width=.1\textwidth]{star1}
\end{center}
\clearpage
\end{titlingpage}
\frontmatter

\tableofcontents
\mainmatter

\chapter{Introduction}

Since the late 1960s, our---at times evolving---understanding of the properties and interactions of the fundamental particles has been summarised by the Standard Model. An overview of a selection of these properties and interactions is given in fig.~\ref{SMsum}.

\begin{figure}[htp]
\begin{minipage}[b]{.74\textwidth}
\input{figures/smover}
\end{minipage}
\hfill\begin{minipage}[b]{.25\textwidth}
\caption{An overview of the particles of the Standard Model. The particles are arranged by mass and charge. Colour indicates particle type, the filling of the border indicates the spin of particles and lines are drawn between those particles that the Standard Model describes interactions between. The currently known maximum bounds on neutrino masses have been used to place the neutrinos in the mass direction. Table values from \cite{wikism}.\label{SMsum}}
\end{minipage}
\end{figure}

In its current form, the Standard Model makes no attempt to explain any physics beyond this.\footnote{The overview in figure~\ref{SMsum} includes massive neutrinos, which have been found experimentally, even though the Standard Model does not at present include them. There are, however, several proposed methods of extending the SM to do so.} The most obvious missing element is gravity, which continues to resist grand unification. Other missing elements include a number of phenomena from cosmology, such as dark matter and dark energy, which may or may not be explained by particles or forces that a complete theory of fundamental particles and forces can be expected to include.

Within these limits, the Standard Model has been remarkably successful, withstanding decades of experimental tests, correctly predicting the existence and properties of a number of particles.\footnote{Most recently, the existence of the Higgs boson was confirmed experimentally. At the time of writing, confirmation of its predicted properties is still a work in progress.} Those successes not withstanding, there are some issues within the Standard Model.

As it is formulated, the SM depends on at least 19 numerical constants,\footnote{Not counting any additional constants needed to account for neutrino masses.} the value of which must be determined experimentally, since the model offers no insight into the origin of or relations between these constants. Worse still, as formulated in the SM, higher order corrections will tend to increase the Higgs mass, with no constraint save the Planck energy. This is one example of the hierarchy problem. The implication is that either some unknown physics exist between the Higgs mass scale and the Planck scale to constrain the Higgs mass, or the bare mass and couplings of the Higgs boson is very finely tuned to cancel the higher order contributions.

In the first case, we will obviously want to search for evidence of the unknown mechanism. In the latter case, we might expect there to be some underlying mechanism that ensures that the bare Higgs mass and the other free parameters of the SM have the proper value. Again, we will want to search for physics outside the SM, as a clue to what that underlying mechanism is.

There is also the possibility that neither of those mechanisms exist, since the Standard Model, strictly speaking, does not require them. In that case, searching for, and not finding any, new physics is still a valuable, albeit less illuminating, result.

In this thesis, we shall approach the task of searching for physics beyond the Standard Model by introducing to it an extension via the effective Lagrangian approach. Specifically, we will introduce a $q\bar q\rightarrow\gamma\gamma$ point interaction, and then simulating collision experiments with the new interaction at various strengths, to see how the outcome is affected. We can then, finally, compare the results of those pseudoexperiments to the results of actual collision experiments performed at CERN's Large Hadron Collider, and look for the same effects there.

\chapter{Theory}

While the detailed procedure for going from a general notion of expanding the Standard Model to creating a specific set of pseudoexperiments with which to compare experimental results are not part of the main thrust of this thesis, and will in any case be handled by various software tools in practice, what will follow is a brief overview of that process.

Since the new interaction will be introduced into the SM by the effective Lagrangian approach, the Lagrangian formulation of the Standard Model as a quantum field theory will be the starting point.

\section{The Lagrangian formulation of QFT}
In classical mechanics \cite{goldstein}, the Lagrangian formulation describes the path taken by a system between a given initial and final state---a particle with an initial and a final position, say---by finding the path that minimises the action $S$, which is defined as the integral along a given path over the Lagrangian $L$:
\[S[q]=\int_\textrm{path}dtL[q,\dot{q}],\]
where $q$ is a generalised coordinate. In this picture, the Lagrangian encapsulates the dynamics of the system. It is related to the Hamiltonian $H$ by
\(L = p\dot q- H,\label{htol}\)
where $p$ is momentum.

In quantum mechanics, the picture of a system travelling along a single, well-defined path from an initial to a final configuration no longer applies. In stead, a probability of going from an initial state $\ket{q}$ to a final state $\ket{q\prime}$ can be found as the absolute square of the transition amplitude\footnote{At this point, we should note that the common notation where $\hbar = c = 1$ will be used from this chapter onwards.} \cite{sred:tramp}
\[A=\bra{q\prime}e^{-i\hat H(t\prime-t)}\ket{q\phantom\prime},\]
where $\hat H$ is the Hamiltonian of the system. Since the idea of a singular path for the system was abandoned, in stead imagine the system travelling along each possible path simultaneously, each with its own transition amplitude. The total transition amplitude, then, is the sum of all the individual transition amplitudes. This can be connected to the classical case by supposing that, for a system with a classical limit, the transition amplitudes of paths close to the classical path will tend to amplify one another, while paths far from it will tend to cancel out.

Through some notational gymnastics, which involve carving the path integral into an infinite number of time steps, each integrated over every possible configuration, and imposing some conditions on the Lagrangian, it can be shown \cite{sred:tramp} that the expression above can be written as
\(A=\int\,\mathcal Dq\,\exp\left[i\int_t^{t^\prime}dt\,[p(t)\dot q(t)-H(p(t),q(t))]\right],\label{e.Dq}\)
where the integral is over all paths with position $q$ at time $t$ and position $q\prime$ at time $t\prime$. We recognise the expression in the innermost integral from eq.~\eqref{htol}.

For a local theory, it is possible to write the Lagrangian as a spatial integral over the Lagrangian density:
\[L=\int \,d^3x\,\mathcal L.\]
Thus, the action can be written as
\(S=\int\,d^4x\,\mathcal L,\label{e.S}\)
which, unlike the previous expression for $S$, is manifestly Lorenz invariant, so long as $\mathcal L$ is a Lorenz scalar. Given the ubiquity of local quantum field theories, it is common in quantum field theory to drop `density' from the name, and refer to $\mathcal L$ as the Lagrangian.

Finally, to get the field theory aspect, replace the generalised coordinate $q$ with a field configuration ``coordinate'' $\phi(x)$, which depends on the Lorenz vector $x$. In short, \eqref{e.Dq} can then be written as
\(A=\int\mathcal D\phi\, e^{iS[\phi]}.\label{e.Dphi}\)

As was the case in classical mechanics, the behaviour of a theory is fully described by its Lagrangian (density), and several models can be combined by adding together their respective Lagrangians. So it is that the Standard Model is described by the SM Lagrangian $\mathcal L_{SM}$, which can be considered as a sum of several, more specific, Lagrangians that describe the separate sectors of the SM. However, before we venture too deeply into the Standard Model, we shall first consider an alternate way of looking at the content of the Lagrangian.

\section{Feynman diagrams}
When studying individual processes described by a theory, Feynman diagrams are a useful tool. So useful, in fact, that much of the software developed to simulate processes is designed around them. To see how they work, consider the simple model described by the Lagrangian
\[\mathcal L= \half\partial^\mu\phi\partial_\mu\phi-\half m^2\phi^2-\frac{\lambda}{4!}\phi^4=\phi[\partial^2-m^2]\phi-\frac{\lambda}{4!}\phi^4\]
This is an example of a $\phi^4$ theory. The first terms in this Lagrangian, which involves two $\phi$s, describes a field propagating into another field, and the other one, which involves 4 $\phi$s, describes and interaction between four fields.

The goal will be to calculate the transition amplitude for a state $\ket{\phi_a}$ going to some other state $\ket{\phi_A}$. One procedure for doing so, which is inspired by \cite{wiki.feydiag}, is to start by writing the state as
\[\ket{\phi}=\int\frac{d^4k}{(2\pi)^4}\tilde\phi(k)\ket{k},\]
where $k$ is a four-momentum. Using eq.~\eqref{e.Dphi}, the transition amplitude can be expressed as
\(A\propto\int\frac{d^4k_A}{(2\pi)^4}\frac{d^4k_a}{(2\pi)^4}\int\mathcal D\phi\,\phi(k_A)\phi(k_a)e^{iS}.\label{transa}\)

To express $S$ in terms of momenta, we go back to the Lagrangian and express $\phi$ in terms of its Fourier modes:
\[\phi(x)=\int\frac{d^4k}{(2\pi)^4}e^{ikx}\tilde\phi(k)\]
The Lagrangian is now
\begin{align*}\mathcal L=&-\int\frac{d^4k}{(2\pi)^4}\frac{d^4k\prime}{(2\pi)^4} e^{i(k+k\prime)x}\tilde\phi(k)[kk\prime +m^2]\tilde\phi(k\prime)\\
&-\frac{\lambda}{4!}\int\frac{d^4p_1\,d^4p_2\,d^4p_3\,d^4p_4}{(2\pi)^{16}}e^{i(p_1+p_2+p_3+p_4)x}\tilde\phi(p_1)\tilde\phi(p_2)\tilde\phi(p_3)\tilde\phi(p_4).
\end{align*}
Inserting this into eq.~\eqref{e.S}, it becomes clear that $x$ only appears as a phase factor, which means that integrating over $x$ only produces delta functions:
\begin{align*}
S=&\int\frac{d^4k}{(2\pi)^4}\tilde\phi(-k)(k^2-m^2)\tilde\phi(k)\\
&-\frac{\lambda}{4!}\int\frac{d^4p_1\,d^4p_2\,d^4p_3\,d^4p_4}{(2\pi)^{16}}\tilde\phi(p_1)\tilde\phi(p_2)\tilde\phi(p_3)\tilde\phi(p_4)\delta(p_1+p_2+p_3+p_4),
\end{align*}
where, in the first term, the delta function identified $k\prime=-k$. The first term of the action describes the free part of the theory, and the second term describes the interacting part, so we will call them $S_F$ and $S_I$, respectively. Using this expression in place of $S$, we can Taylor expand in $\lambda$:
\[e^{iS}=e^{i(S_F+S_I)}=e^{iS_F}\left(1-iS_I+\frac{(-iS_I)^2}{2}+\frac{(-iS_I)^3}{3!}+\frac{(-iS_I)^4}{4!}+\cdots\right)\]
This assumes that $\lambda$ is small enough to make the interaction merely a pertubation of the theory.


Inserting this back into eq.~\eqref{transa}, we get an expression for the transition amplitude expanded in powers of $\lambda$. If we call these terms $A_n$, so that $A\propto\sum_{n=0}^\infty A_n$, the first term of the expansion is
\[A_0=\int\frac{d^4k_A}{(2\pi)^4}\frac{d^4k_a}{(2\pi)^4}\underbrace{\int\mathcal D\phi\,\phi(k_A)\phi(k_a)e^{iS_F}}_{D_0}.\]
Looking more closely at the part labelled $D_0$ above, we can expand it to find that
\[D_0=\int\mathcal D\phi\,\phi(k_A)\phi(k_a)e^{\int\frac{d^4k}{(2\pi)^4}\phi(k)[k^2-m^2]\phi(-k)},\]
which is a Gaussian (functional) integral \cite{armbjorn}:
\[\int d^nx\,x^k\cdots x^{2N}e^{-\half x^iA_{ij}x^j}.\label{gausint}\]
As such, the integral has the following solution, provided that the participating momenta are identical:
\[D_0=\half\frac{\delta^4(k_a-k_A)}{k^2-m^2}\int\mathcal D\phi\,e^{iS_F},\]
where the delta function is introduced to ensure that the momenta are identical, as required. Introducing this delta function is equivalent to imposing momentum conservation. The remaining integral over the free action corresponds to the vacuum $0\rightarrow0$ process in free theory, and is a constant with respect to $\phi$. This constant can be interpreted as the vacuum energy content of all space, which we will nevertheless simply divide out:
\[A\propto\int\frac{d^4k_A}{(2\pi)^4}\frac{d^4k_a}{(2\pi)^4}\,\frac{\int\mathcal D\phi\,\phi(k_A)\phi(k_a)e^{iS}}{\int\mathcal D\phi\,e^{iS_F}}\]
With this, we find that 
\[D_0=\frac{\delta^4(k_a-k_A)}{k^2-m^2},\]
which is the propagator in momentum space.


Were we to carry out the momentum integrations over $D_0$, there would evidently be a singularity at $k^2=m^2$. This singularity can be avoided by slightly modifying the integration path. There are several ways of doing this, including Feynman's prescription, which yields the expression $1/(k^2-m^2+i\epsilon)$, the Feynman propagator in momentum space. 


The second term in the expansion is
\[A_1=\int\frac{d^4k_A}{(2\pi)^4}\frac{d^4k_a}{(2\pi)^4}\left(\prod_{n=1}^4\frac{d^4p_n}{(2\pi)^4}\right)\,D_1,\]
where\footnote{Here, $\phi^n$ is shorthand for a product of $n$ $\tilde\phi$ functions of separate momenta.}
\[D_1=-\frac{i\lambda}{4!}\delta^4(p_1+p_2+p_3+p_4)\frac{\int\mathcal D\phi\,\phi^6e^{iS_F}}{\int\mathcal D\phi\,e^{iS_F}}.\] 
Solving the Gaussian integral tell us that $D_1$ is equal to a sum of terms of the form 
\(-\ono{3!2^3}\frac{i\lambda}{4!}\delta^4(p_1+p_2+p_3+p_4)\frac{\delta^4(k_1-p_1)}{{k_1}^2-m^2}\frac{\delta^4(p_2-p_3)}{{p_2}^2-m^2}\frac{\delta^4(p_4-k_A)}{{k_A}^2-m^2},\label{1t1f}\)
where the momenta are paired in all possible combinations. There are $6!$ different combinations, but they fall into only two groups of topologically inequivalent diagrams, as illustrated in figure~\ref{fey1t1o2}.


\begin{figure}[htb]
\begin{minipage}{.65\textwidth}
\begin{footnotesize}\begin{center}
\begin{tikzpicture}
\draw (-3,.2) node[left]{$a$} -- 
      ++(1,0) to[in=45,out=135,min distance=15mm,looseness=8] ++(0,0) --
      ++(1,0) node[right]{$A$};
\draw (1,.2) node[left]{$a$} -- ++(2,0) node[right]{$A$} 
      ++(-1,.5) to[in=45,out=-45,min distance=15mm,looseness=8] 
      ++(0,0) to[in=225,out=135,min distance=15mm,looseness=8] ++(0,0);
\end{tikzpicture}
\end{center}\end{footnotesize}
\end{minipage}
\hfill
\begin{minipage}{.3\textwidth}
\begin{center}\begin{footnotesize}
\begin{tikzpicture}
\draw (-1,0) node[left]{$a$} -- (1,0) node[right]{$A$};
\end{tikzpicture}
\end{footnotesize}\end{center}
\end{minipage}
\begin{minipage}[t]{.65\textwidth}
\caption{The two topologically inequivalent diagrams that can be constructed from the second term in the expansion of the $1\rightarrow1$ transition amplitude in eq.~\eqref{1t1f}. They are drawn by imagining the ingoing and outgoing states as labelled points, and drawing a line from one of these states to the state that its momentum is set equal to by the delta function in a propagator. The delta function over four momenta from the $\phi^4$ term is drawn as a vertex where exactly four lines come together.\label{fey1t1o2}
}
\end{minipage}
\hfill
\begin{minipage}[t]{.3\textwidth}
\caption{The first term drawn with the same method as fig.~\ref{fey1t1o2}.\label{fey1t1o1}}
\end{minipage}
\end{figure}


This way of illustrating the terms is called a Feynman diagram. The single, somewhat less complicated, Feynman diagram associated with the first term in the expansion is shown in fig.~\ref{fey1t1o1}.

By topologically inequivalent diagrams, we mean diagrams that cannot be rearranged to be identical with one another without disconnecting a line from a vertex\footnote{In the parlance of the topic, two topologies are inequivalent when there does not exist a continuous map that takes one into the other. However, to properly define all the terns in the previous sentence, we would need to venture beyond the scope of this thesis.}. In this context the ingoing and outgoing state labels are attached to the external lines. This becomes important when working with more in- and outgoing states and/or more vertices.

To count how may of each of the diagrams in fig.~\ref{fey1t1o2} there are, we note that there are $4!$ different ways of connecting lines to a vertex, each of the three lines can be drawn in both directions, giving a factor $2^3$, and the lines may be drawn in any order, giving a factor $3!$. These factors cancel out the factor $1/(3!2^3)$ from the solution of the Gaussian integral and the factor $1/4!$ from the $\phi^4$ term in the Lagrangian. Sadly, this way of counting diagrams doesn't give the correct number. In the left diagram, the factor 2 related to reversing the direction of the looping line counts diagrams that were already included in the $4!$ ways of connecting the vertex. In the right diagram, this is true for both of the looping lines, along with an additional factor 2 counting the order in which the looping lines are drawn, which was also already included in the factor $4!$ associated with the vertex, for a total overcounting of a factor $8$. These overcountings arise whenever there are symmetries in the diagrams, which allow us to reach the same diagram by two different manipulations. Note, though, that the symmetries that are responsible for these overcountings are apparent just by looking at it.

Looking at eq.~\eqref{1t1f}, one of the internal $p$ momenta can not be fixed to the external $k$ momenta, which leaves this term proportional to a diverging integral, associated with the looping line in the left diagram in fig.~\ref{fey1t1o2}. There are established methods for renormalising these divergent terms, however for the present purposes, we note simply that for any process, there will among the lowest order terms that describe it be a tree level\footnote{In graph theory, a tree is a connected, loop-free graph.} diagram, which is then the leading order diagram for that process. Because we are working in a pertubative regime by assumption, loop-level diagrams of that process will act as higher order corrections to that leading order term.

Knowing all this, it should be clear that enough information is available in a Feynman diagram to allow us to recover the terms in the expression for the transition amplitude that it represents. That being the case, it is possible to reverse the procedure by which we found the Feynman diagrams in the first place, and extract the expression for the transition amplitude from a set of Feynman diagrams. We can even start with constructing the Feynman diagrams, and recover the expression from them, thus bypassing the derivation above.


\begin{figure}[htb]
\hfill
\begin{tikzpicture}
\draw (-3,1) -- (-1,-1) (-3,-1) -- (-1,1);
\node[right] at (-1,0) {$=-i\lambda$};
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\node[right] at (-1,0) {$=\dfrac{1}{k^2-m^2}$};
\draw (-3,0) -- (-1,0);
\node at (0,-1) {};
\node at (0,1) {};
\end{tikzpicture}
\hfill \phantom{d}
\caption{The building blocks for Feynman diagrams in $\phi^4$ theory. Once constructed, find the momentum of each propagator by imposing momentum conservation at each vertex. Any momentum that cannot be related to one of the external momenta is integrated over.
\label{phi4rules}}
\end{figure}


The rules for constructing the Feynman diagrams---the Feynman rules---are fairly straightforward.

\begin{enumerate}
\item Construct all topologically inequivalent diagrams in which the ingoing and outgoing states for the process in question and the proper number of vertices for the present order in $\lambda$ are connected by propagator lines.
\item Impose momentum conservation at all vertices and across all propagators. As was already noted once, this is equivalent to introducing delta functions over the momenta.
\item Determine the symmetry factor for each diagram.
\item Construct for each diagram its value by taking the product of the values for each element of the diagram from figure~\ref{phi4rules}. Integrate over all momenta that have not been related to external momenta with measure $d^4p/(2\pi)^4$. Divide by the symmetry factor associated with the diagram.
\end{enumerate}


For example, the $2\rightarrow2$ transition amplitude to zeroth order in $\lambda$ is given by the Feynman diagrams in figure~\ref{efeydig1}.

\begin{figure}[htp]
\begin{footnotesize}\begin{center}
\begin{tikzpicture}
\draw (-4,1) node[left]{$a$} -- (-2,1) node[right]{$A$};
\draw (-4,-1) node[left]{$b$} -- (-2,-1) node[right]{$B$};
\draw (0,0) node{{\normalsize $+$}};
\draw (2,1) node[left]{$a$} -- (4,-1) node[right]{$B$};
\draw[line width=5pt,white] (2,-1) -- (4,1) ;
\draw (2,-1) node[left]{$b$} -- (4,1) node[right]{$A$};
\end{tikzpicture}
\end{center}\end{footnotesize}
\caption{The Feynman diagrams associated with the first term in the expansion of the $2\rightarrow2$ transition amplitude. Once again, connected states are required to have the same momentum, however with more particles going into and coming out of the process, there are more than one way of connecting the ingoing and outgoing states.
\label{efeydig1}}
\end{figure}

The value of these diagrams, using the rules, is
\[\frac{\delta^4(k_1-k_A)}{{k_1}^2-m^2}\frac{\delta^4(k_2-k_B)}{{k_2}^2-m^2}+\frac{\delta^4(k_1-k_B)}{{k_1}^2-m^2}\frac{\delta^4(k_2-k_A)}{{k_2}^2-m^2},\]
which is indeed what we would get from solving the Gaussian integral. At the next order in $\lambda$, we can build the diagrams in fig.~\ref{efeydig2}.

\begin{figure}[htp]
\begin{footnotesize}
\begin{minipage}{.09\textwidth}
\normalsize \hfill
\end{minipage}
\begin{minipage}{.9\textwidth}
\begin{center}
\begin{tikzpicture}[scale=.75]
\draw (-6.5,0) to [in=225,out=315,min distance=25mm,looseness=8] (-6.5,0) to [in=135,out=45,min distance=25mm,looseness=8] (-6.5,0);
\draw (-5,0) node{\normalsize$\times$};
\draw (-4,0) node{$\left(\text{\tikz[scale=.75] \draw (0,1) (0,-1);}\right.$};
\draw (-3,1) node[left]{$a$} -- (-1,1) node[right]{$A$};
\draw (-3,-1) node[left]{$b$} -- (-1,-1) node[right]{$B$};
\draw (0,0) node{{\normalsize $+$}};
\draw (1,1) node[left]{$a$} -- (3,-1) node[right]{$B$};
\draw[line width=5pt,white] (1,-1) -- (3,1) ;
\draw (1,-1) node[left]{$b$} -- (3,1) node[right]{$A$};
\draw (4,0) node{$\left)\text{\tikz[scale=.75] \draw (0,1) (0,-1);}\right.$};
\draw (5.5,0);
\end{tikzpicture}
\end{center}
\end{minipage}

\vspace{.5em}

\begin{minipage}{.09\textwidth}
\normalsize $+$
\end{minipage}
\begin{minipage}{.9\textwidth}
\begin{center}
\begin{tikzpicture}[scale=.75]
\draw (1,1) node[left]{$a$} -- ++(2,0) node[right]{$A$}
      ++(-2,-2) node[left]{$b$} -- 
      ++(1,0) to[in=45,out=135,min distance=15mm,looseness=8] ++(0,0) --
      ++(1,0) node[right]{$B$};
\draw (4,0) node{\normalsize $+$};
\draw (5,1) node[left]{$a$} -- 
      ++(1,0) to[in=225,out=315,min distance=15mm,looseness=8] ++(0,0) --
      ++(1,0) node[right]{$A$}
      ++(-2,-2) node[left]{$b$} -- 
      ++(2,0) node[right]{$B$};
\draw (8,0) node{\normalsize $+$};
\draw (9,1) node(p1)[left]{$a$} -- ++(2,-2) node[right]{$B$};
\draw[line width=5pt,white] (p1.east) ++(0,-2) -- ++(2,2);
\draw (p1.east) ++(0,-2) node[left]{$b$} --
      +(.5,.5) to[in=180,out=90,min distance=15mm,looseness=8] +(0.5,0.5) --
      ++(1.5,1.5) node[right]{$A$};
\draw (12,0) node{\normalsize $+$};
\draw (13,1) node(p2)[left]{$a$} -- 
      +(.5,-.5) to[in=180,out=270,min distance=15mm,looseness=8] +(0.5,-0.5) --
      ++(1.5,-1.5) node[right]{$B$};
\draw[line width=5pt,white] (p2.east) ++(0,-2) -- ++(2,2);
\draw (p2.east) ++(0,-2) node[left]{$b$} --
      ++(2,2) node[right]{$A$};
\end{tikzpicture}
\end{center}
\end{minipage}

\vspace{.5em}

\begin{minipage}{.09\textwidth}
\normalsize $+$
\end{minipage}
\begin{minipage}{.9\textwidth}
\begin{center}
\begin{tikzpicture}[scale=.75]
\draw (1,1) node[left]{$a$} -- (3,-1) node[right]{$B$};
\draw (1,-1) node[left]{$b$} -- (3,1) node[right]{$A$};
\end{tikzpicture}
\end{center}
\end{minipage}

\end{footnotesize}
\caption{The Feynman diagrams associated with the second term in the expansion of the $2\rightarrow2$ transition amplitude. The joining of four momenta by the last delta function is illustrated by having four lines meet in a point.
\label{efeydig2}}
\end{figure}

Of these diagrams, those in the first two lines are simply those of fig.~\ref{efeydig1} with loops added, and can be considered the one-loop, or next-to leading order, part of those processes.
 The last diagram is the only one that we have not seen before, and it introduces a new feature. In all the diagrams we have examined so far, momentum conservation has required that one of the final states be exactly identical to one of the initial states. Not so in the final diagram, where the delta function at the vertex only requires that the sum of momenta, here defined so that positive momenta flow toward the vertex, is zero. In $S$-matrix notation, where the transition matrix $S$, which transits an initial state into a final state, can be written as
\[S=1+iT,\]
this last diagram is the first part of the non-trivial $T$-matrix.

As for the disconnected vacuum bubble seen in the top row, and in fig.~\ref{fey1t1o2}, note that the diagram(s) that the vacuum bubble multiplies are the diagrams for the process from the preceeding orders. At higher orders in the expansion, we will find it as a repeated pattern that the diagrams from the previous orders reoccur, multiplied with various combinations of vacuum bubble diagrams. Combining the vacuum bubble contributions on any one diagram at all orders, we find that they can be written as the exponential of the sum of all possible vacuum bubbles \cite{sred:freediagexp}. The same result can be reached by writing
\[\int\mathcal D\phi\,e^{i(S_F+S_I)},\]
the expression for the $0\rightarrow0$ process to all orders. Since this is another constant, we can divide it out like we did with the vacuum normalisation, making the expression for the transition amplitude now
\[A\propto\int\frac{d^4k_A}{(2\pi)^4}\frac{d^4k_a}{(2\pi)^4}\frac{\int\mathcal D\phi\,\phi(k_A)\phi(k_a)e^{iS}}{\int\mathcal D\phi\,e^{iS}}.\]

With that, we find that the non-trivial part of the expression, the $T$-matrix from above, can be found by taking only the connected diagrams---the diagrams in which it is possible to go from any one part to any other along connected lines---into account. Using just this process in the transition amplitude, we can calculate the probability of the system going to some final state specifically through the process described by this diagram. That probability will depend on $\lambda$, which means that if we have a way of distinguishing those events in a detector that went through this process from those that did not---looking at just the diagrams found so far, the fact that the latest diagram allows exchange of momentum between the two particles would make those events stand out from the rest---provided that contributions from even higher order terms do not muddle the picture too much, we would be able to say something about the value of $\lambda$.

The treatment so far obviously deals with a very simple model. The development of the full Standard Model is the subject of entire textbooks \cite{srednicki}, and we will not go into further detail here.

The practical upshot, though, is that the Feynman rules extend to cover the Standard Model by introducing several types of fields, which can be represented in Feynman diagrams by some new styles of lines (dashed, wavy, curled, etc.). Charge is introduced by adding a direction to the lines associated with charged particles---since reversing the charge of a particle is equivalent to reversing the time direction. These new field interact in several new types of vertices, weighted by three coupling constants: the electromagnetic coupling $\alpha_\text{QED}$, the weak coupling constant $\alpha_W$ and the strong coupling constant $\alpha_s$. With this, we can show the Standard Model processes that produce the preponderance of two-photon final states with the two diagrams in figure~\ref{smfeyn}.


\begin{figure}[htb]
\parbox[t]{.45\textwidth}{\begin{center}\begin{footnotesize}\begin{tikzpicture} [>=triangle 45]
\draw[>-] (-1,1) -- (0,1);
\draw[->] (0,1) -- (0,0);
\draw[<-] (-1,-1) -- (0,-1) -- (0,0);
\draw (-2,1) node[left] {$q$} -- (-1,1);
\draw (-2,-1)  node[left] {$\bar q$} -- (-1,-1);
\draw[snake=coil,segment aspect=0] (0,1) -- (2,1) node[right] {$\gamma$};
\draw[snake=coil,segment aspect=0] (0,-1) -- (2,-1) node[right] {$\gamma$}; 
\end{tikzpicture}
\end{footnotesize}\end{center}
\subcaption{SM contribution at tree level. \label{lofeyn}}}\hfill
\parbox[t]{.52\textwidth}{\begin{center}\begin{footnotesize}
\begin{tikzpicture} [>=triangle 45]
\draw[>-] (1,1) node[below]{$q$} -- (2,1);
\draw[decorate, decoration={coil,amplitude=2pt, segment length=2.68pt}] 
    (-2,1) node[left]{$g$} -- (0,1) ;
\draw[decorate, decoration={coil,amplitude=2pt, segment length=2.68pt}] 
    (-2,-1) node[left]{$g$} -- (0,-1); 
\draw[<-] (1,-1) node[above]{$\bar q$} -- (2,-1);
\draw (0,1) -- (1,1);
\draw (0,-1) -- (1,-1);
\draw[-<] (0,1) -- (0,0);
\draw (0,0) -- (0,-1);
\draw[->] (2,1) -- (2,0);
\draw (2,0) -- (2,-1);
\draw[decorate, decoration={snake}] (2,1) -- (4,1) node[right]{$\gamma$};
\draw[decorate, decoration={snake}] (2,-1) -- (4,-1) node[right]{$\gamma$};
\end{tikzpicture}
\end{footnotesize}\end{center}
\subcaption{SM contribution at loop level. $g$s mark gluons.\label{boxdiag}}}\hfill
\caption{ Feynman diagrams for the two leading Standard Model processes that produce a $\gamma\gamma$ final state.\label{smfeyn}}
\end{figure}

We can get a feel for the relative strength of these two diagrams by turning to two sets of simulated collisions available from the ATLAS collaboration.
%\footnote{The internal ATLAS names are \verbatim{mc12_8TeV.129180.Pythia8_AU2CTEQ6L1_gammagamma_2DP20.merge.NTUP_PHOTON.e1199_s1479_s1470_r3542_r3549_p1344} and \verbatim{mc12_8TeV.146800.Pythia8_AU2CTEQ6L1_GamGamBox_pT35pT20.merge.NTUP_PHOTON.e1222_s1469_s1470_r3542_r3549_p1344}.}
Plotted in figure~\ref{boxpart} are the distribution of the invariant masses of photon pairs, defined as \cite{marshaw:invmass}
\begin{align*} 
M_{\gamma\gamma}&=\sqrt{(E_1+E_2)^2+|\mathbf p_1+\mathbf p_2|^2},
\intertext{which, in the case of massless particles, can be rewritten as}
&=\sqrt{2p_1p_2(1-\cos\theta)}. \label{sinvmass}
\end{align*}

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{sffamily}
\input{figures/boxpart}
\end{sffamily}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{The distribution of invariant masses of Standard Model diphoton events as predicted by simulation. The box contribution gives just those events produced by the box diagram in fig.~\ref{boxdiag}. These are ATLAS datasets produced with pythia8 \cite{pythia}, and normalised to the luminosity of the data sample. \label{boxpart}}
\end{minipage}
\end{figure}

\section{The effective Lagrangian approach}

As was established before we ventured in to the world of Feynman diagrams, the SM Lagrangian consists of a sum of terms, each of which describes the behaviour of or interactions between the sectors of the Standard Model. It is no great stretch, then, to consider expanding the Standard Model by adding a new term to the Lagrangian, which describes some new physics. Doing so, however, is not unproblematic.

It is a property of the Standard Model\footnote{[Or it is a requirement on the Standard Model. Claiming that the SM is \emph{inherently} unitary might be something of a stretch.]} that it is unitary, meaning that the total probability of a given state to propagate into any of the possible final states evaluates to 1. Clearly, one cannot simply add any new term to the Standard Model Lagrangian without breaking this unitarity. Rather than going through the painstaking process of ensuring that the new term we will add to the SM preserves its unitarity, we will in stead build on the assumption that new physics exists at high mass scales, and think of the SM Lagrangian as simply the zeroth order term in a series expansion of some larger model. There will then be higher order corrections to the Standard Model, in some mass scale $\Lambda$ that the expansion is performed in. 
In that case, the SM is no longer assumed to be a complete model, and moreover, the expanded model is not even expected to be a complete model to an order in $\Lambda$, which allows us to sidestep the issue of unitarity.
This approach only works if the mass scale is significantly larger than the energies at which the Standard Model is being probed, since the higher order contributions would otherwise have a detectable influence in the lower energy range where the parameters of the SM are determined, meaning that the original assumption of the SM Lagrangian as a zeroth order term in an expansion in $\Lambda$ is no longer valid.

With this assumed plethora of possible higher order terms, a single possible new term can be added without worring about maintaining the integrity of the SM. The specific term considered here, which describes the $q\bar q\rightarrow\gamma\gamma$ contact interaction which is the focus of this thesis, takes the form \cite{rizzo}
\(\mathcal L_n = \frac{2ie^2}{\Lambda^4}Q_q^2F^{\mu\sigma}F^\nu_\sigma\overline{q}\gamma_\mu\partial_\nu q,\label{rizzo}\)
where $e$ is the elementary charge, $Q_q$ is the quark charge of quark $q$ and $\Lambda$, as discussed, is the associated mass scale. The power of $\Lambda$ is chosen to give the correct mass dimension of the term: in natural units,\footnote{Where $\hbar = c = 1$.} the action is unitless. The Lagrangian is integrated over 4 lengths to give the action, which means it must have a length dimension of $-4$ itself. Finally, in natural units, we can equate a unit of length with a unit of inverse mass\footnote{Because in natural units, \[[\text{length}]=[\text{length}]\frac{c}{\hbar}=[\text{length}]\frac{\left[\frac{\text{length}}{\text{time}}\right]}{\left[\frac{\text{length}^2\text{ mass}}{\text{time}}\right]}=\left[\frac{\text{length}^2\text{ time}}{\text{length}^2\text{ time}\text{ mass}}\right]=[\text{mass}]^{-1}. \]}, which means that the Lagrangian must have mass dimension 4. The factors $F^{\mu\sigma}F^\nu_\sigma$ and $\overline{q}\gamma_\mu\partial_\nu q$ each contribute a mass dimension of 4, so the mass scale, which has dimension of mass, must get a power of $-4$.

\begin{figure}[htp]\begin{center}
{\footnotesize\begin{tikzpicture} [>=triangle 45]
\draw[>-] (-1,.5) -- (0,0);
\draw[<-] (-1,-.5) -- (0,0);
\draw (-2,1) node[left] {$u_{ap,2}$} -- (-1,.5);
\draw (-2,-1)  node[left] {$\bar u_{bq,1}$} -- (-1,-.5);
\draw[snake=coil,segment aspect=0,line before snake=3mm] (0,0) -- (2,1) node[right] {$\gamma_{\mu,3}$};
\draw[snake=coil,segment aspect=0,line before snake=3mm] (0,0) -- (2,-1) node[right] {$\gamma_{\nu,4}$}; 
\node at (1,0) [right] {\footnotesize$=\dfrac{8 e{}^2}{9 \Lambda^4} \delta_{p q} \big(p_2^\rho p_3^\rho p_4^\sigma g^{\mu \nu} \gamma_{a b}^\sigma -p_2^\mu p_3^\nu p_4^\rho \gamma_{a b}^\rho -p_2^\rho p_3^\rho p_4^\mu \gamma_{a b}^\nu +p_2^\mu p_3^\rho p_4^\rho \gamma_{a b}^\nu $};
\node at (2.3,-.6) [right] {$+p_2^\rho p_3^\sigma p_4^\rho g^{\mu \nu} \gamma_{a b}^\sigma -p_2^\nu p_3^\rho p_4^\mu \gamma_{a b}^\rho -p_2^\rho p_3^\nu p_4^\rho \gamma_{a b}^\mu +p_2^\nu p_3^\rho p_4^\rho \gamma_{a b}^\mu \big)$};
\end{tikzpicture}
}\end{center}
\caption{An example of a Feynman rule as created by entering the new term from eq.~\eqref{rizzo} into LanHEP\cite{lanhep}. It gives the coupling constant of the four-point interaction between an up- and an antiup-quark, and two photons. $p_n$ represents the four-momentum of particle $n$, $\gamma_{ab}^\mu$ is the $\gamma$-matrix, $g^{\mu\nu}$ is the metric tensor, $\delta_{pq}$ is the Kronecker delta, $e$ is the elementary charge and $\Lambda$ is the associated mass scale.\label{rule}}
\end{figure}

The effect of the new term is to introduce several Feynman rules of the type shown in fig.~\ref{rule}.

\begin{new}
These new processes may interfere constructively or destructively with the Standard Model contributions to this process. The effects of both on the distribution of invariant masses of photon pairs are illustrated in figure~\ref{interf}.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny \makebox[0pt][l]{
\hspace{-1em}\input{figures/interference}
}\end{infilsf} \end{minipage}
\hfill\begin{minipage}[b]{.3\textwidth}
\caption{The effect on the distribution of the invariant masses of the produced photon pairs of introducing the new term into the Lagrangian at various values of the mass scale $\Lambda$, assuming constructive (non-grayed) and destructive (grayed) interference with the SM contribution. Note that the distributions that assume destructive interference produce fewer events than those that assume constructive interference at the same value of $\Lambda$. These Monte Carlo samples were produced with CalcHEP.
\label{interf}}
\end{minipage}
\end{figure}

Given that the distribution of invariant masses contain more events in the sensitive region if we assume constructive interference, a lower bound on the value of $\Lambda$ that we discover while using this assumption will lie below the lower bound that we would find, had we assumed destructive interference. Therefore, we will move forward assuming that the new term interferes constructively with the Standard Model.
\end{new}

One possible interpretation of this new four-point interaction is as a zero-range approximation of a process like the one shown in fig.~\ref{across}, involving some unknown mediating particle \cite{marshaw:zerorange}.

\begin{figure}[htb]
\parbox[t]{.45\textwidth}{\begin{center}\begin{footnotesize}\begin{tikzpicture} [>=triangle 45]
\draw[>-] (-1,.5) -- (0,0);
\draw[<-] (-1,-.5) -- (0,0);
\draw (-2,1) node[left] {$q$} -- (-1,.5);
\draw (-2,-1)  node[left] {$\bar q$} -- (-1,-.5);
\draw[snake=coil,segment aspect=0] (0,0) -- (2,1) node[right] {$\gamma$};
\draw[snake=coil,segment aspect=0] (0,0) -- (2,-1) node[right] {$\gamma$}; 
\end{tikzpicture}
\end{footnotesize}\end{center}
\subcaption{Point $q\bar q \gamma\gamma$ interaction.\label{cross}}}\hfill
\parbox[t]{.45\textwidth}{\begin{center}\begin{footnotesize}
\begin{tikzpicture} [>=triangle 45]
\draw[->] (-2,1) node[left] {$q$} -- (-1.5,.5);
\draw[-<] (-2,-1) node[left] {$\bar q$}  -- (-1.5,-.5);
\draw (-1.5,.5) -- (-1,0);
\draw (-1.5,-.5) -- (-1,0);
\draw[dashed] (-1,0) -- node[below] {$X$} (0,0);
\draw[snake=coil, segment aspect=0] (0,0) -- (1,1)node[right] {$\gamma$};
\draw[snake=coil, segment aspect=0] (0,0) -- (1,-1)node[right] {$\gamma$};
\end{tikzpicture}
\end{footnotesize}\end{center}
\subcaption{The $q\bar q \gamma\gamma$ interaction with mediating particle $X$.\label{across}}}\hfill
\caption{Feynman diagrams of the relevant contact interaction. (a) is the interaction described by the new term in the Lagrangian, while (b) is the type of interaction this can be considered a zero-range approximation of.\label{feyns}}
\end{figure}


\section{The cross section}
Using the methods developed so far, we can calculate the transition amplitudes, and hence the probabilities, associated with single processes. However, by long standing tradition, particle physics is interested in the cross section $\sigma$.

To understand the cross section, consider as an analogy the case of firing a single projectile at a single target. We might at this point imagine an arrow and a bulls-eye or an electron and an atomic nucleus in a piece of gold film. The probability of hitting the target is then the cross sectional area of the target divided by the cross sectional area $A$ of the space where the projectile might fly. Adding the possibility of a number, $N_P$, of projectiles being fired at a number, $N_T$, of different, non-overlapping targets, the number, $N$, of hits is calculated as
\[N=\frac{N_P N_T \sigma}{A}.\]
If we apply this picture to a quantum mechanical system, we are mixing the kinematic probability for two particles coming close enough to interact with the dynamic probability of a particular interaction occurring. We can fix this by expressing the probability $\mathcal P$ as a function of the separation between the interacting particles, given by the impact parameter $\mathbf b$, a 2-D vector, and then integrating over all $\mathbf b$. Then, we can write
\[\sigma=\int d^2b\,\mathcal P(\mathbf b)\propto\mathcal P(\text{in}\rightarrow\text{out})=\left|\braket{\phi\cdots\phi_\text{out}}{\phi\cdots\phi_\text{in}}\right|^2.\]
In this way, we can calculate the cross section of some process from its transition amplitude.

From an experimental point of view, $N_P$, $N_T$ and $A$ all depend on the immediate conditions within the accelerator. They can be combined into the luminosity $\mathscr L$, which in a proton collider contains information about how many protons are brought to an interaction point at a time, and how densely they are packed. By multiplying the luminosity with the cross section of a process, we get the frequency with which that process occurs in the detector. Integrating the luminosity over time, we get the integrated luminosity, which can be thought of as a measure of how many opportunities for interactions there have been over the period of time being integrated over, independent of the fine details of how the experiment was run\footnote{One notable example of a non-fine detail of an experiment: beam energy.}.

\section{Colliding protons \label{sec.pdfth}}
In the processes described so far, the starting point has been the interaction of a quark and an antiquark. And while being able to single out such a process experimentally would certainly be nice, single quarks sadly do not occur in nature. Because quakrs are colour charged particles, they are subject a phenomenon known as colour confinement, which requires that colour charge always occur in bundles which are colour neutral when viewed from the outside. For our purposes, protons are an abundant, stable and easy-to-handle colour-neutral bundle of quarks and gluons, which we use in collisions in place of the naked quarks that the problem formulation really calls for. This way of thinking obviously completely ignore the several advantages that a hadron collider possesses.

\begin{figure}[htp]
\includegraphics[width=.55\textwidth]{pdf}\hfill\parbox[b]{.44\textwidth}{
\caption{Parton distribution function obtained from the \textsc{cteq} collaboration. It gives the probability of extracting a specific quark from a proton with a certain fraction, $x$ of its energy. From \cite{scien2}.\label{pdff}}}
\end{figure}

While protons contain no antiquarks as valence quarks, every proton is surrounded by a `sea' of virtual particles. At a given energy scale $Q^2$, there is a certain probability for extracting one of these sea quarks in an interaction, given by the Parton Distribution Functions (PDFs), a selection of which is shown in fig.~\ref{pdff}. These functions are found experimentally by several collaborations. This thesis will deal mainly with the CTEQ set of PDFs.

Including this step in the calculation leads to the following expression, which gives the cross section for a diphoton event resulting from a proton-proton collision, $\sigma(pp\rightarrow\gamma\gamma)$ in terms of the $\sigma(q\bar q \rightarrow \gamma\gamma)$ cross section that we have determined thus far:
\[\sigma(pp\rightarrow\gamma\gamma)=\sum_q\iint dx_1\,dx_2\,f_q(x_1,Q^2)f_{\bar q}(x_2,Q^2)\sigma(q\bar q\rightarrow\gamma\gamma),\label{pdf}\]
where $f_q$ is the PDF for parton $q$.


\chapter{Simulation studies}\label{ch.mc}

Now that the theoretical description of the processes under study is in place, we might suppose that obtaining a prediction of the distributions of events that an experiment will find is a simple matter of carrying out the integrals laid out in the previous chapter. Unfortunately, these integrals defy analytical solution, which leaves only the option of integrating numerically. Specifically, we will use the Monte Carlo method for numerical integration, which prescribes inserting a random value drawn from a suitable distribution into the integrand in place of the variable of integration, and then calculating the value of the integrand. Each result estimates the value of the integral, and by averaging several such estimates, an incrementally better estimate is obtained.

Since all of the variables that are integrated over have physical significance, choosing specific values for the variables of integration can be viewed as equivalent to laying out the kinematics of a single hypothetical event. Repeatedly estimating the value of the integral a sufficient number of times to obtain a result of acceptable accuracy from the Monte Carlo integration process in effect leaves us with a large set of such simulated events. In particle physics, this process goes by Monte Carlo simulation, and the software packages that are designed to carry out these calculations are called event generators.

\section{Event generators}

The event generator used for the bulk of the work in this thesis is CalcHEP \cite{calchep}. Other choices of event generators include MadGraph \cite{madgraph5} and pythia \cite{pythia}. To illustrate the behaviour of these different generators with respect to one another, figure~\ref{evgen} plots distributions of invariant mass ($M_{\gamma\gamma}$) and transverse momentum ($p_T$) of a set of simulated events generated by each one.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\hspace{-1ex}\makebox[0pt][l]{\input{figures/genspt2}}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\subcaption{The distribution of $p_T^{\gamma_1}$, the transverse momentum of the leading photon, in the event samples produced by three event generators. There is a general trend toward pythia producing more events and MadGraph producing fewer events than CalcHEP, however these distributions are still compatible with one another within their errors.}

\phantom{p}
\end{minipage}
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\hspace{-1ex}\makebox[0pt][l]{\input{figures/gensmgg}}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\subcaption{The distribution of $M_{\gamma\gamma},$ the invariant mass of the photon pair, in the event samples produced by three event generoatrs. pythia's tendency to produce more events is less pronounced in this distribution, however MadGraph's trend toward fewer events is more pronounced, especially in the region with high statistics between 1000 and 1500 GeV, where the number of events produced by MadGraph and CalcHEP differ by as much as 4 standard deviations over several bins.}

\phantom{p}
\end{minipage}
\caption{Plots showing the distributions of $p_T$ and $M_{\gamma\gamma}$ of events generated by the three event generators \cite{calchep} \cite{pythia} \cite{madgraph5}. Below, the ratio plots were created by dividing the content in each bin of the distributions with the content of the corresponding bin in the distribution for the CalcHEP sample. The errors on the bins were derived through standard error propagation. All these samples were produced using stratified sampling, where seperate sets of events were produced for different ranges of $p_T$, which is why the error bars fluctuate in size somewhat. The MadGraph sample shows a clear bias, which indicates a systematic uncertainty. Note though, that the MadGraph samples were not produced by the author, who cannot verify that the production setting match those used in the other event generators. Nevertheless, a systematic uncertainty of [something] due to the event generator selection will be included.
\label{evgen}}
\end{figure}

These plots, and those to follow in this chapter, owe a lot to the analysis validation software RIVET\footnote{\textbf{R}obust \textbf{I}ndependent \textbf{V}alidation of \textbf{E}xperiment and \textbf{T}heory.} \cite{rivet}.

By default, the energy scale $Q^2$, on which the running of the strong coupling constant, $\alpha_S$, depends, is defined differently in CalcHEP than it is in pythia and MadGraph. To achieve the result in figure~\ref{evgen}, this default was changed to 
\[Q^2=\frac{p_T(\gamma_1)^2+p_T(\gamma_2)^2}{2},\]
which matches the setting in the other two event generators.

As figure~\ref{evgen} illustrates, the event sample generated by MadGraph show a significant bias compared to the other samples. This sample was not produced by the author, which makes investigation into the cause of this bias difficult. For the present analysis, the bias is included as a systematic uncertainty, however it remains a possibility that this uncertainty could be reduced or eliminated by a more careful study.


\section{Feynman rule calculators}
The event generators introduced above assemble events based on sets of Feynman rules (in a computerised format), so to generate events from a model that includes the new contact interaction, the new term in the Lagrangian that describes the contact interaction must be expressed as a set of Feynman rules (in the correct computerised format). As was hinted at when the Feynman diagrams that describes the new interaction were introduced in figure~\ref{rule}, there exist automated ways of carrying out this conversion. For this thesis, the tool used was LanHEP \cite{lanhep}, which was designed as a companion to CalcHEP's antecedant CompHEP, and as such, it and CalcHEP interface readily. Alternatives exist, however, such as the Mathematica package FeynRules \cite{feynrules}. 

LanHEP takes as input a Lagrangian written in a format similar to \LaTeX's math language, and produces a set of Feynman rules usable by CalcHEP.

With the ability to produce sets of events with varying values of $\Lambda$, the mass scale of the contact interaction, the opportunity presents itself to create simulated distributions of events for several potential observables with which we might discriminate the effects of the contact interaction. Figure~\ref{discr} capitalises on this opportunity.

\begin{figure}[htp]
\begin{minipage}[b]{.499\textwidth}
\begin{infilsf}\tiny
\hspace{-.9em}\makebox[.96\textwidth]{\input{figures/sigcos}}
\end{infilsf}
\subcaption{Significance: 30.4 \label{sigcos}}
\end{minipage}
\hfill
\begin{minipage}[b]{.499\textwidth}
\begin{infilsf} \tiny
\makebox[.96\textwidth]{\input{figures/sigpt}}
\end{infilsf}
\subcaption{Significance: 671.5}
\end{minipage}
\begin{minipage}[t]{.499\textwidth}
\begin{infilsf} \tiny \phantom{p}

\vspace{-1em}
\makebox[.96\textwidth]{\input{figures/sigmgg} }
\end{infilsf}
\subcaption{Significance: 971.5}
\end{minipage}\hfill
\begin{minipage}[t]{.45\textwidth}
\caption{Three different observables that could potentially be used to discriminate between models: the cosine of $\theta_{\gamma_1}$, the scattering angle of the leading photon (measured, in this case, in the Collins-Soper frame \cite{collinssoper}), $p_T^{\gamma_1}$, the transverse momentum of the leading (most energetic) photon and $M_{\gamma\gamma}$, the invariant mass of the photon pair. Two samples are generated with CalcHEP, one for the Standard Model case, and one for a contact interaction with a mass scale $\Lambda = 1.0$ TeV. The significance is calculated as the difference in bin content between the two distributions divided by the root-sum-square of the errors on the bins, summed over all bins where both distributions have non-zero content.
\label{discr}}
\end{minipage}
\end{figure}


Given the significances quoted in that figure, $M_{\gamma\gamma}$, the invariant mass, is the obvious choice for a discriminating variable. It has the additional advantage over the other two methods that it does not depend on identifying the leading---most energetic---photon of the pair. In a truth sample such as this, making such an identification does not present a problem, however when considering the effects on a photon of passing through the material of the detector, it becomes problematic to claim that the photon that leaves the largest energy deposit in the calorimeter is also the photon that left the hard event with the greatest amount of energy.

One argument against the measure of significance used in figure~\ref{discr} is that it rewards observables where both distributions have many non-zero bins, which is the result of a choice made about how the data is represented, rather than something intrinsic to the data. However, since the per-bin differences are weighted by the error on each bin, we expect that a distribution with a single bin with a very significant difference between bin contents is scored similarly to a distribution with a large number of bins with relatively insignificant differences between bin contents. This speaks in this method's favour, since we could create a distribution like the first one by combining the bins of the latter distribution, assuming the differences between the distributions are in the same direction for all the bins.

Backed by these considerations, we will use invariant mass as the discriminating variable going forward.

[Note to self: sum of $|p_T|$? Hvis du kommer til at kede dig...]

Meanwhile, figure~\ref{sigcos} shows no discernible difference between the SM sample and the sample generated with a 1 TeV mass scale contact interaction. Samples generated at lower mass scales do show a significant difference, although it changes only the scale, and not the shape, of the distribution. [Possibly a plot?]

\section{Parton Distribution Functions}
As described in section~\ref{sec.pdfth}, we use a set of experimentally determined functions called Parton Distribution Functions (PDFs) to describe the probability of extracting a given parton from a proton. Given that the PDFs are not exact analytical models, we must account for the uncertainty associated with the method by which  PDFs are determined. To estimate that uncertainty, we compare the distribution in invariant masses of events generated by CalcHEP using the CTEQ6 set of PDFs, which are the events that will be used moving forward, with events generated using the alternative MRST2002nlo set of PDF, which is the only alternative PDF available in CalcHEP.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf}
\hspace{-.8cm}\input{figures/mrst}
\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{Comparing the distribution of invariant masses of events produced by CalcHEP with the CTEQ6 (grayed) and MRST2002nlo (non-grayed) PDFs at different values of $\Lambda$. This is to give an idea of the systematic uncertainty on this distribution due to the choice of PDF. Table~\ref{mrsttab} summarizes the fractional variation in predicted events by $\Lambda$ and invariant mass range. The invariant mass ranges used, indicated by the dashed lines in this plot, are discussed in the text. \label{mrst}}
\end{minipage}
\end{figure}
\begin{table}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf}{\footnotesize
\begin{center}
\arrayrulecolor{natgreen}
\begin{tabular}[b]{cr!{\color{white}|}r!{\color{white}|}r!{\color{white}|}r}\hline
&&\multicolumn{3}{c}{ \color{natgreen}{\bfseries $M_{\gamma\gamma}$ range [GeV]} } \\
&&\multicolumn{1}{c!{\color{white}\vrule}}{\bfseries [100:1000)} & \multicolumn{1}{c!{\color{white}\vrule}}{\bfseries [1000:3000)} & \multicolumn{1}{c}{\bfseries [3000:5000)} \\ %\cline{3-5}
& \textbf{0.75} & 16.9 \% & 11.1 \% & 31.9 \% \\
&\textbf{1.00} & 16.3 \% & 7.7 \% & 32.2 \% \\
\multirow{-3}{*}{\rotatebox[origin=c]{90}{\color{natgreen}{\bfseries $\Lambda$ [TeV]}}} &\textbf{$\infty$} & 16.8 \% & 3.4 \% & 51.1 \%\\\hline
\end{tabular}
\end{center}}\end{infilsf}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{The fractional deviation between the distribution of simulated events produced by CalcHEP with the CTEQ6 PDF versus the MSRT2002nlo PDF plotted in fig~\ref{mrst}. This will form one of the systematic uncertainties on the final result. \label{mrsttab}}
\end{minipage}
\end{table}

The resulting distributions are plotted in fig~\ref{mrst}, and the difference between the sample generated using the CTEQ6 PDF set and the MRST2002 PDF set are quantified in table~\ref{mrsttab}, broken up into three invariant mass ranges: a low range with a great deal of statistics and very little separation between the three models, a middle range with good statistics and good separation between the models, and a high range where statistics start to run out, especially for the SM sample. These deviations will be included in the analysis as a systematic uncertainty.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{figures/Zep-soft}
\caption{An illustration of the processes that may surround an interesting event in a proton-proton collision, and the steps required to arrive at a final particle content of that event. In this figure, the dark gray blobs represent the incoming protons and the large red blob represents a hard quark-quark interaction. As illustrated here, the products of such a hard interaction may carry colour charge as well, in which case these products too would need to undergo hadronisation, which, in turn, would show up in the detector as jets. However, since the end product of the process currently under study is photons, which are colourless, that will not be the case here. The upper portion of the figure illustrates the evolution of a secondary gluon interaction, which, though a number of gluon emissions, produces a selection of quark final states that must be gathered into colourless clusters and finally into hadrons. Both stages may undergo further evolution or decay during the process. The final collection of hadrons, which for a single parton interaction will be reasonably well collimated, form a jet of particles in the detector. Similar jet-forming processes may also attach to particles emitted as initial or final state radiation. This figure reproduced from \cite{zep}. For further details on these surrounding processes and their computational representation, see eg \cite{pythman}.
\label{zep}}
\end{figure}

\section{Extended event}
The events produced by the event generator(s) represent the physical process that occurs in the point where two protons interact. Given the limits on our ability to see such interactions, we need to exptend the scope of physical processes in the simulation, to the point where the resulting event information represents something that we might realistically pick up with a detector.

The initial and final particle states are fixed in the event generator, however in reality, both initial and final states might easily undergo decay or emission before or after the event occurs, altering the particle content and kinematics of the event. Such initial and final state radiation must be modelled to give an accurate picture of what a detector might see.

Further, final state particles with colour charge will not remain isolated due to colour confinement. These particles will develop a jet of other coloured particles, so that the colour charge becomes obscured to outside observation. The simulation of the process by which colour charged particles combine into colour neutral hadrons is simulated is called hadronisation. Since we are dealing with photons in the final state in the present analysis, this step is not crucial in the events generated to study this process specifically, however $\pi^0$ mesons, one of the major backgrounds to the photon signal, are produced in this way. 

A schematic summary of these processes, along with the steps that the simulation software that models these processes will usually be divided into, is presented in figure~\ref{zep}. In this thesis, the extension of the hard events provided by CalcHEP with these surrounding processes will be carried out in pythia8 \cite{pythia}. Figure~\ref{pythify} illustrates the effect that these surrounding processes have on the distribution of invariant masses.

\begin{figure}[htp]
\centering
\begin{minipage}[b]{.69\textwidth}\hspace{-1.5em}\makebox[0pt][l]{
\noindent\begin{infilsf}
\tiny
\input{figures/bfpyth}
\end{infilsf}}
\end{minipage}\hfill
%\begin{minipage}[b]{.\textwidth}
\caption{The distribution of invariant masses in the event samples generated by CalcHEP at three values of $\Lambda$, and the same event samples after extending them in surrounding processes with pythia, with ratio plots. Once again, the effect of the use of stratified sampling is visible as a jump in the magnitude of the erros around 1000 GeV. The ratio plots make it clear that the effect on this particular observable of creating the extended events is simply to remove a fraction of the events. This is reasonable, since the effect of adding final state radiatio to a hard event is to alter the final state particle content for a fraction of those events, and since we require two final state photons to calculate an invariant mass, final states that do not contain two photons are discarded. We can conclude from this figure that the selection of events with altered final state particle content does not depend on the invariant mass of the photons in the event, nor does the process of extending the event alter the distribution of photon invariant masses.
\label{pythify}}
%\end{minipage}
\end{figure}

This constant difference in distributions between the hard and extended event samples does not apply for all observables. Figure~\ref{pythicos} illustrates how the distribution of cos $\theta_{\gamma_1}$, the scattering angle for the leading photon is skewed slightly toward lower values, meaning a greater proportion of large scattering angles. This could be an effect of extended events having their kinematics altered so that the leading photon in the extended event is not the same as the leading photon in the hard event, so that the subleading photon, which scatter in the opposite direction from the leading photon, spill into the leading photon sample. This would also explain why the invariant mass sample, which is not sensitive to the identification of a leading photon, is not affected.

\begin{figure}[htp]
\begin{minipage}[b]{.65\textwidth}
\begin{infilsf} \tiny \makebox[0pt][l]{
\hspace{-1em}\input{figures/pythicos}
}\end{infilsf}
\end{minipage}
\hfill\begin{minipage}[b]{.3\textwidth}
\caption{A plot of the distribution of the cosine of the scattering angle $\theta$ (in the CS frame, as in fig.~\ref{discr}) of the leading photon in the set of events generated by CalcHEP in the SM scenario before (green) and after (red) being extended with pythia. Aside from the same constant faction of lost events as was also seen in fig.~\ref{pythify}, we note that the distribution contains events with lower $\cos\theta_{\gamma_1}$ than were present in the set of generated events.
\label{pythicos}}
\end{minipage}
\end{figure}

Since the main process under study does not involve coloured final states, we do not expect that these surrounding processes will be a significant source of systematic uncertainty or bias, We are supported in this assumption by the findings in figure~\ref{pythify}. To further support that expectation we might attempt to add the extended processes with a different software package, however [that would require us to spend time figuring out how such a thing works...]

\section{Detector simulation}
At this point, we have a precise picture of every detail of a number of hypothetical events. In that way, it is very unlike the picture that the detector will give us of the events that actually happen in the experiment.

The actual events will be viewed through the `lens' of the ATLAS detector, which only sees the particles passing through it as energy deposits in various detector elements, and the computer algorithms used to reconstruct the particles that caused those deposits\footnote{This description of the ATLAS detector will be expanded upon in chapter~\ref{ch.exp}.}. Additionally, the particles found as the end product of the interactions by pythia can potentially interact with the material that makes up the detector, and decay, split or convert before or during their detection, and the detector may not fully or accurately detect all the particles that pass through it.

Because of this, it is necessary to also simulate how the detector interacts with the final state particles we found in the previous section, to properly relate that information with experimental data. Additionally, since the same software that reconstructs particle information from detector data is used to reconstruct simulated particle information from simulated detector data, the end product of the simulated events will be in the same format, containing the same variables, as the data available from the experiment.

Once this final step is completed, we will have a set of hypothetical events as we imagine they would look if they were to happen in the experiment---a set of pseudo-experiments.

Since creating such pseudo-experiments is necessary for a large portion of analyses that wish to make use of ATLAS data, the ATLAS collaboration has created a (reasonably) standardised set of programs for carrying out this final step in creating them, called GEANT4~\cite{geant4}.

The real detector has a finite energy resolution, which is not reflected in the Monte Carlo events, for which we have an exact momentum. To overcome this, we smear the energies given by the simulation---that is shift the energies by a random amount chosen from a gaussian distribution with a width chosen to reflect the resolution of the detector.

The effect that the detector simulation has on the invariant mass distribution of the event samples created above is shown in figure~\ref{geant-beaf}.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf} \tiny
\input{figures/geant-beaf}
\end{infilsf}
\end{minipage}
\hfill\begin{minipage}[b]{.3\textwidth}
\caption{A plot comparing the event samples produced by CalcHEP and pythia with the results of running those event through the detector simulation available through GEANT4. We note a clear trend toward fewer reconstructed photon pairs at higher invariant masses. \label{geant-beaf}}
\end{minipage}
\end{figure}

Alternatively, we might extrapolate a systematic description of how the detector affects distributions of events from performing this simulation on several event sets, and apply that description to any new distribution of events as a simple correction[, and if we were going to compare the two methods, this would be the place for it...]

\section{Pileup}

Which is a thing. More information is forthcoming. Probably.

Or maybe it isn't a thing. Who knows? Excitement!

\chapter{Experiment}\label{ch.exp}

Knowing now what we expect to see in an experiment, given the models used, we now move to describing how we are able to see those phenomena.

It can come as no surprise, given the title, that this thesis will use the \textsc{atlas}\footnote{\textbf{A} \textbf{T}oroidal \textbf{L}HC \textbf{A}pparatu\textbf{s}.} experiment, which is part of the Large Hadron Collider complex at \textsc{cern}\footnote{\textbf{C}onseil \textbf{E}uropéen pour la \textbf{R}echerche \textbf{N}ucléaire. When the council tasked with creating the european nuclear research laboratory became the organisation tasked with running that laboratory, its name changed to Organisation Européenne pour la Recherche Nucléaire---the European Organisation for Nuclear Research, but the acronym stayed the same. Acronyms, it seems, are not only ubiquitous, but also immutable.} in Geneva.

\section{The Large Hadron Collider}

The purpose of the Large Hadron Collider is to accelerate protons\footnote{Or heavy ions, on the occations when the LHC runs heavy ion collision experiments.} (which are a species of hadrons) to very high kinetic energies, and then bringing them to collide with other protons, which have similarly been accelerated to very high energies. As a necessity dictated by have gone to these very high energies, the accelerator is also quite large.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=.8\textwidth]{Cernrings}
\end{center}
\begin{minipage}[b]{\textwidth}
\caption{A schematic view of the \textsc{cern} accelerator complex \cite{cernbro}, which accelerates the protons or ions used in collision experiments through several accelerators to progressively higher energies. The paths protons can take through the machine are marked with light gray triangles. The dark gray triangles mark the paths taken by lead ions when the Collider runs proton--lead or lead--lead collision experiments. Protons are `created' by ionising hydrogen atoms and then injected by \textcolor{Purple}{LINAC 2} into the \textcolor{Plum}{Booster ring}. From there, protons are accelerated by the Proton Synchrotron (\textcolor{Magenta}{PS}) and then the Super Protron Synchrotron (\textcolor{RoyalBlue}{SPS}) before finally being sent into the \textcolor{MidnightBlue}{LHC} ring. The LHC ring is the largest one, measuring approximately 27~km in circumference.}
\label{cernrings}
\end{minipage}
\end{figure}

The protons are accelerated to the appropriate energies by electric fields in radiofrequency cavities. They are then kept confined within the two beam pipes in the LHC ring---one for each of the two beams moving in either direction---by superconducting multipole magnets. The LHC does not, however, accelerate protons all the way from rest to its collision energy. It is the last part of a multi-stage machine, which is shown in schematic form in fig.~\ref{cernrings}. As an interesting aside, two of the intermediate stages in this machine, the Proton Synchrotron and the Super Proton Synchrotron were at one point \textsc{cern}'s main accelerator.

At the LHC, the beam energy was intended to be 7 TeV per beam, however several accidents when the machine was started has forced us to lower out ambitions for the time being. In 2012, when the data that will be used in this thesis was taken, the LHC ran at 4 TeV per beam, for a total collision energy of 8 TeV.

The two beam pipes cross another at four points around the LHC ring, the centres of the four detectors.

\section{The ATLAS detector}
The \textsc{atlas} detector sits at one of these interaction points, and is, along with CMS\footnote{The \textbf{C}ompact \textbf{M}uon \textbf{S}olenoid.}, a general purpose detector, designed to capture as much information as possible about collision events. To that end, the \textsc{atlas} detector uses three distinct types of detectors, arranged in concentric layers around the interaction point. From innermost to outermost, these are: the tracking system, the calorimeter system and the muon tracking system. The layout of the detector is shown in figure~\ref{allatlas}. For a more detailed description, see \cite{detectorpaper}.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\includegraphics[width=1\textwidth]{AllAtlasBig}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{A diagram of the entire \textsc{atlas} detector \cite{atlasweb}. The overall structure is of a layered cylinder centred on the interaction point. We refer to those parts of the detector that make up the wall of the cylinder as the barrel section, and to the ends of the cylinder as the endcap. The electromagnetic calorimeter, which is the detector element that we will make the most use of here, is coloured orange in this drawing.}
\end{minipage}
\label{allatlas}
\end{figure}

\textsc{Atlas} defines its own coordinate system, centred on the interaction point, where the position in the angular direction, perpendicular to the beam pipe, is measured by the azimuthal coordinate $\phi$, and the angle to the beam pipe is measured in pseudorapidity $\eta$, which is defined as
\(\eta=-\ln[\tan(\theta/2)],\)
where $\theta$ is the polar angle to the beam pipe in radians \cite{green:eta}. The pseudorapidity $\eta$ is a simple transformation of $\theta$---it is 0 at $\theta=\pi/2$, $\infty$ at $\theta=0$ and $-\infty$ at $\theta=\pi$---but is chosen for its similarity to rapidity, $y$, which is additive under Lorenz boosts \cite{green:y}.

Of the three detector types, the most important to the search for photons is the electromagnetic calorimeter.

The muon tracker forms the outermost, and most voluminous, part of the detector, since muons are one of only a small number of particles that can penetrate through the calorimeters on a regular basis. Neutrinos will do so as well, however, capturing neutrinos with \textsc{atlas}' mere 90~000~t of material and 7~000 m$^3$ detector volume \cite{atlasweb} is something of a lost cause. As is also the case with the inner detector, the muon detector localises the track of a passing charged particle by picking up the ionisation trail it leaves in a sensitive material. Also like the inner detector, it ascertains the momentum and charge of passing charged particles by measuring their deflection by an applied magnetic field.

The inner detector uses two distinct detector technologies to localise the tracks of charged particles that pass through it. The innermost layers use silicon semiconductor chips to detect the charge left from a track, and is a compact, high--precision system. The outer part of the inner detector uses drift straws---which pick up the ionisation charge left in an inert gas by attracting it to a wire with a high voltage charge in the centre of the straw---to more economically, in terms of material, readout complexity and expense, cover a larger volume.

Both the straw detector and the outer part of the silicon detector, the scilicon microstrip detector, have very long detector elements, which can only report that a hit has occurred somewhere along its length. To improve resolution the direciton parallel to these long detectors, successive layers of detector elements are placed at an angle to one another. The innermost layer of the silicon detector, the pixel detector, uses a pixel structure rather than a strip structure, and so has adequate resolution in all directions. With these detectors, readout from \atlas{} only consists of a list of detectors that have been hit. Particle tracks are reconstructed from these hits in the subsequent analysis of an event through fitting.

There are two calorimeter systems in \textsc{atlas}: the (inner) electromagnetic calorimeter and the (outer) hadronic calorimeter. Both function by stopping the particles that pass through them, and then measuring the energy they deposit. All calorimeters in \textsc{atlas} are sampling calorimeters, meaning that the absorbing material has sensitive layers inserted into it at intervals. These sensitive layers measure how deep into the absorbing material the particles penetrate, and thus how much energy the carried before entering the calorimeter. In the barrel section of the hadronic calorimeter, the sensitive layers are scintillators, a material that luminesces when exposed to ionising radiation, and the absorbing layers are iron. The remaining calorimeters, which covers both the EM calorimeters and the endcap hadronic calorimeters, have liquid argon as the sensitive material. As with the straw detectors, activity in the liquid argon layers are detected by an electrode, which picks up the ionisation left by passing energetic particles. The barrel section of the calorimeter has lead as an absorber, held in place by thin steel layers, however, in the endcap calorimeters, which are subjected to a stronger particle flux, the lead is replaced with copper, and in some cases tungsten, which is easier to cool.

\subsection{The electromagnetic calorimeter}

The EM calorimeter is the most important tool for detecting photons. As such, we devote an entire sub--section to describing it.

\begin{figure}[htp]
\begin{minipage}[b]{.59\textwidth}
    \begin{center}
    \includegraphics[width=\textwidth]{larpic}\makebox[0em][r]{\textcolor{natgreen}{\rule{\textwidth}{1pt}}}
    \end{center}
\end{minipage}
\begin{minipage}[b]{.4\textwidth}
    \begin{center}
    \includegraphics[width=\textwidth]{shower}\makebox[0em][r]{\textcolor{natgreen}{\rule{\textwidth}{1pt}}}
    \end{center}
\end{minipage}

\begin{minipage}[t]{.59\textwidth}
    \begin{center}
    \subcaption{A section of the LAr calorimeter.}
    \end{center}
\end{minipage}
~\begin{minipage}[t]{.4\textwidth}
    \begin{center}
    \subcaption{Illustration of a particle shower within the LAr calorimeter. \label{shower}}
    \end{center}
\end{minipage}

\begin{minipage}[b]{.59\textwidth}
    \begin{center}
    \includegraphics[width=1\textwidth]{figures/CaloDiag.png}
    \subcaption{Schematic showing the placement of the LAr calorimeters in \textsc{atlas}.}
    \end{center}
\end{minipage}
\begin{minipage}[b]{.4\textwidth}
\caption{Several figures \cite{atlasweb} that illustrate the design and functioning of the LAr calorimeters. These are sampling calorimeters, which have an absorbing medium (lead or steel) with layers of detecting medium (liquid argon) inserted regularly to keep track of the developing shower shape. In \textsc{atlas}, rather than having flat layers, the absorbing ans sensitive materials are interleaved in an accordion shape, visible in (a) and (b), which allows the detector electronics to be placed on the surface of the absorbing plates, rather than interrupting the calorimeter with non-sensitive signalling equipment.
\label{calostruc}}
\end{minipage}
\end{figure}

The iron and lead absorbing material in these calorimeters are constructed in an accordion shape, with liquid argon filling the gaps. The shape is visible in fig.~\ref{calostruc}. Plates carrying a high voltage are suspended in the middle of the space between accordion plates, which attract the ionised trail left by the passage of high energy charged particles, and detect the charge they deposit. The accordion shape allows readout electronics to run continuously out of the detector, rather than being interspersed as discreet layers in the absorbing material. This layout removes the need to interrupt the detector with non--sensitive communication channels.

For our purposes, the interesting quantities will be the energy deposited by photons and electrons. In the presence of matter, high--energy photons loose energy primarily through pair production, while electrons loose energy primarily through bremsstrahlung. The typical lengths travelled by both types of particle before undergoing such a process---their radiation lengths---depend on the material traversed, but are roughly equal to one another \cite{fernow:sampcal}. As illustrated in fig.~\ref{emshower}, both types of particles will undergo the same sort of evolution as they travel through an absorbing material, splitting into pairs of particles, each with a fraction of the energy of the parent particle, until the daughter particles fall below the energy where they can be absorbed by the material. This cascade of particles develops in a shower structure, as illustrated in fig.~\ref{shower}. Thus, we can measure the energy of the original particle both by how deep into the absorbing material it penetrates, or how many daughter particles it produces. \textsc{Atlas}' LAr calorimeters mainly use the second strategy.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\begin{infilsf}\footnotesize
\input{figures/emcasc}
\end{infilsf}
\end{minipage}\hfill
\begin{minipage}[b]{.3\textwidth}
\caption{A schematic description of an EM shower developing in an absorbing material. Here, a wavy line indicates a photon, a straight line indicates an electron, and a straight line with a `$+$' a positron. At each split, the two resulting particles carry away half the energy of the original particle. In a sampling calorimeter, a sensitive layer is typically inserted at intervals of one radiation length. \textsc{Atlas}' LAr calorimeters measure the magnitude of ionisation that is left in the liquid argon by the passage of the particle shower. Adapted from \cite{fernow:sampcal}.
\label{emshower}
}
\end{minipage}
\end{figure}

The shower can be initiated by material in the detector ahead of the calorimeter just as well as the material in the calorimeter. To account for this possibility, the first active layer, called the presampler, sits ahead of the first absorbing layer. Photons that undergo pair production sufficiently deep in the detector for the tracker to resolve at least one of the (anti--)electrons that are produced are treated as a separate object type, namely converted photons.

The calorimeter is divided into three layers, which are split into readout bins as illustrated in fig.~\ref{caldiv}. The first layer is very finely divided in the $\eta$ direction, and its readout cells will on occasion be referred to as `strips' in the `strip layer', as opposed to the `cells' in the other two layers. The energy deposited in the calorimeter is measured by the magnitude of the ionisation left in the active layers by the particle showers.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\includegraphics[width=\textwidth]{caldiv}
\end{minipage}
\begin{minipage}[b]{.3\textwidth}
\caption{The division of the EM calorimeter into detecting cells \cite{egede}. The first layer is divided into thin strips for the greatest resolution in the $\eta$ direction. The second layer is divided into roughly square cells, and comprises the bulk of the depth of the detector. The last layer is presumed to only be reached by the most energetic particles, and can have a coarser division without loosing resolution. This diagram is of the calorimeter at $\eta = 0$, closest to the interaction point. At higher $|\eta|$, the towers are angled so that they are still pointed toward the interaction point.
\label{caldiv}}
\end{minipage}
\end{figure}

The division of the calorimeter into layers gives us some resolution in depth when attempting to describe the shape of a shower. Showers initiated by different types of particles evolve in a slightly different way, which allows us discern the source of a shower by examining its shape. Additionally, with this shower shape information, we can extrapolate the direction from which a particle entered the calorimeter by looking at the shape of the shower, which is of particular importance when attempting to trace the origin of unconverted photons, which otherwise leave no trail in the detector.

\subsection{Photon identification}
The detector, then, provides information about clusters of energy deposits in the calorimeter and tracks reconstructed from hits in the tracking detector, we are tasked with identifying those signals that have been left by photons.

We can build a set of candidates for deposits left by unconverted photons by taking those deposits in the calorimeter that do not have a track pointing to them. We expect converted photons to show up as pairs of electron tracks in the tracking detector that share a vertex away from the interaction point. However, we must account for the possibility that only one of the tracks are reconstructed, and so we also consider electrons tracks that only show hits in the straw detector as candidates for converted photons. These selection rules can then be written into a computer algorithm, which performs the event--by--event sorting for us \cite{phorec}.

This set will of course be contaminated with events that were not caused by photons: $\pi^0$ mesons also create calorimeter hits with no associated track, and QCD jets can mimic converted photons, among other possibilities. To attempt to sort genuine photon events from impostor events, we study the shape of the shower left in the calorimeter. To that end, we define the following shower shape variables \cite{Carminati}:

\begin{itemize}
\item $R_\text{had}$, the ratio of energy deposited in the hadronic calorimeter to the cluster energy in the EM calorimeter. Hadronic showers are expected to penetrate deeper into the hadronic calormieter than EM showers.
\item In the middle EM calorimeter layer, non-EM showers spread wider than electromagnetic ones. The variables that measure the shape of the shower in this layer are
\begin{itemize}
\item $R_\eta$, the ratio in $\eta$ of cell energies in 3 $\times$ 7 versus 7 $\times$ 7 cells.
\item $R_\phi$, the ratio in $\phi$ of cell energies in 3 $\times$ 7 versus 7 $\times$ 7 cells.
\item $w_{\eta 2}$, the width of the shower in the $\eta$ direction.
\end{itemize}
\item The strip layer, with its greater resolution in $\eta$, can pick out some of the internal structure of a jet. Hadron showers tend to show more than one maximum. Variables that measure the shape in the strip layer are
\begin{itemize}
\item $w_{s3}$, the shower width for three strips around the maximum strip.
\item $w_{s\text{ tot}}$, the total lateral shower width in the strip layer.
\item $F_\text{side}$, the faction of energy deposited outside a core of 3 central strips, but within 7 strips.
\item $\Delta E$, the difference in energy of the strip with the second largest energy deposited and the strip with the smallest energy deposited between the two leading strips.
\item $E_\text{ratio}$, the ratio of the energy difference associated with the
largest and second largest energy deposits, over the sum of these energies.
\end{itemize}
\end{itemize}

These variables form the selection criteria by which we declare a photon candidate to be an actual photon. The full list of variables above forms the tight selection criteria, while a shorter list, consisting of $R_\text{had}$, $R_\eta$ and $w_{\eta2}$ form the loose selection criteria. Separate cut values in these variables exist for different $\eta$ ranges. For a complete description, see \cite{Carminati}.

\subsection{Triggering and data collection}
While in full operation for the 2012, 8~TeV run, the LHC delivered a bunch crossing in \textsc{atlas}' interaction point every 50~ns. Reading out the whole detector produces 1.6~MB of information, which, if the detector were read out completely with every crossing, would produce a data rate of 34~TB/s.\footnote{For perspective, that is approximately equal to the estimated global IP traffic rate in 2015, according to \cite{wolframip}.} However, since only a fraction of these collisions produce interesting physics events, we can reduce the data rate to less prohibitive levels simply by not recording data from collision that do not produce interesting events. To accomplish this, we need a system that examines events in the detector as they occur, and trigger recording whenever it sees an interesting event. In \textsc{atlas}, this trigger system has three levels \cite{detectorpaper}.

The level--1 trigger genuinely does examine events as they occur in the detector. To do so, it runs on specialised hardware built in to the detectors, and as a result, it only has access to the raw information from the detectors to which it is attached. This means, for example, that track reconstruction is not available when the level--1 trigger deicdes whether or not to record an event. For this analysis, which looks for diphotons, we will use events that passed the \texttt{2g40\_loose} level--1 trigger, which requires that the EM calorimeter reports two hits with at least 40~GeV of transverse energy that pass the loose selection criteria.

The next two trigger levels have access to more information and, crucially, more time to compute derived quantities in the reduced number of events that come through the level--1 trigger. With this, the event selection can be further refined, to the point where the final event rate peaks at 600 events per second, and averges 300 events per second.

This limit to the readout rate can, unfortunately, not be reached by only removing uninteresting events. To stay within the limitations of the readout system, \atlas{} removes a fraction of the events that did pass the triggers, when they originate from a trigger that produces more events than it is considered worth keeping.\footnote{Explaining how it is decided whether data is worth keeping would veer into a discussion of \atlas{} internal politics, which is a topic beyond the scope of this thesis.} The diphoton channel is important to the search for the Higgs boson, however, so the triggers that produce diphotons events are not prescaled in this fashion.


\chapter{Data}
The ATLAS data that will be used for this thesis was taken in 2012, during the 8~TeV run of the LHC, during which ATLAS recorded 20.3~fb$^{1}$ of integrated luminosity.

\begin{figure}[htp]
\begin{minipage}[b]{.69\textwidth}
\hspace{-1em}\includegraphics[width=\textwidth]{figures/intlumi}
\end{minipage}\hfill\begin{minipage}[b]{.3\textwidth}
\caption{A plot showing the integrated luminosity delivered by the LHC (green), recorded by ATLAS (yellow), and fulfilling data quality criteria (blue), over the course of the 8 TeV run in 2012 \cite{publiclumi}.
\label{intlumi}}
\end{minipage}
\end{figure}

Something, something, photon container, skim etc. etc. 18301.3~nb$^{-1}$.

A significant fraction of this data will be made up of events that are not relevant to the present analysis, so to reduce the data set to a managable size, the unneeded events are skimmed away.

\section{Skimming}


\chapter{Analysis}
All of the bits so far are put together, and a limit on the size of
$\Lambda$ is found.

\chapter{Conclusion}
The found limit is re-iterated, and the caveats on its prescisioin are
listed. Implications are gone through, and proposals for improvements
are made.

\renewcommand{\bibname}{References}
\bibliographystyle{plainurl}
\bibliography{cite}

\end{english}
\end{document}
